{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNDL_Dataset_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LT1lVyVFcxAi",
        "RVad8u3lc32c",
        "ExAjZnVTdC9A",
        "Uf8Et2IIgB5S",
        "N0enYqduhZBf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V9N1R71Lpbv"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense,Embedding,LSTM,GRU,Flatten,Dropout, Input, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
        "from keras.layers import Concatenate, Activation\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnlCehOb2pYi",
        "outputId": "394cac5d-886a-4cf0-b8c8-f92495f87b54"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvi_HNsQomrK"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT1lVyVFcxAi"
      },
      "source": [
        "# Importing IMDb Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DRqP-Hum9eb",
        "outputId": "c35dc3b9-7155-4948-9b93-07fa158fd6aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuNdy5frXn3K"
      },
      "source": [
        "df_train = pd.read_csv(\"drive/MyDrive/NNDL_Datasets/IMDB_train.csv\")\n",
        "df_val = pd.read_csv(\"drive/MyDrive/NNDL_Datasets/IMDB_val.csv\")\n",
        "df_test = pd.read_csv(\"drive/MyDrive/NNDL_Datasets/IMDB_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xP7FskpaflQ",
        "outputId": "6026c289-6ca1-4406-a298-56a32c3039a1"
      },
      "source": [
        "len(df_train), len(df_val), len(df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35000, 7500, 7500)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVad8u3lc32c"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7-7dVeKZCKL"
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def text_preprocessing(dataframe):\n",
        "\n",
        "  lines = dataframe[\"clean_text\"].values.tolist()\n",
        "  reviews = list()\n",
        "\n",
        "  for line in lines:\n",
        "      tokens = word_tokenize(line)\n",
        "      tokens = [w.lower() for w in tokens]\n",
        "      table = str.maketrans(\"\",\"\",string.punctuation)\n",
        "      stripped = [w.translate(table) for w in tokens]\n",
        "      words = [w for w in stripped if w.isalpha()]\n",
        "      words = [w for w in words if w not in stop_words]\n",
        "      reviews.append(words)\n",
        "\n",
        "  reviews = [' '.join(review) for review in reviews]\n",
        "  dataframe['prepro_rev'] = reviews\n",
        "  return dataframe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhYxnNmGZgd-"
      },
      "source": [
        "df_train = text_preprocessing(df_train)\n",
        "df_val = text_preprocessing(df_val)\n",
        "df_test = text_preprocessing(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrycH9KMLvLN"
      },
      "source": [
        "# lines = df[\"clean_text\"].values.tolist()\n",
        "# stop_words = set(stopwords.words(\"english\"))\n",
        "# reviews = list()\n",
        "# for line in lines:\n",
        "#     tokens = word_tokenize(line)\n",
        "#     tokens = [w.lower() for w in tokens]\n",
        "#     table = str.maketrans(\"\",\"\",string.punctuation)\n",
        "#     stripped = [w.translate(table) for w in tokens]\n",
        "#     words = [w for w in stripped if w.isalpha()]\n",
        "#     words = [w for w in words if w not in stop_words]\n",
        "#     reviews.append(words)\n",
        "\n",
        "# reviews = [' '.join(review) for review in reviews]\n",
        "# df['prepro_rev'] = reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1kgGi62KpCY"
      },
      "source": [
        "# train_df = df[df['data_type'] == 'train']\n",
        "# test_df = df[df['data_type'] == 'val']\n",
        "\n",
        "train_text = df_train['prepro_rev'].values\n",
        "val_text = df_val['prepro_rev'].values\n",
        "test_text = df_test['prepro_rev'].values\n",
        "\n",
        "train_text = [s.lower() for s in train_text]\n",
        "val_text = [s.lower() for s in val_text]\n",
        "test_text = [s.lower() for s in test_text]\n",
        "\n",
        "train_classes = df_train['labels'].values\n",
        "val_classes = df_val['labels'].values\n",
        "test_classes = df_test['labels'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExAjZnVTdC9A"
      },
      "source": [
        "### Tokenizer for Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-01L7LNdVR2"
      },
      "source": [
        "EMBED_DIM_WORD = 768\n",
        "MAX_NUM_WORDS_WORD = 30522\n",
        "MAX_SEQUENCE_LENGTH_WORD = 400\n",
        "do_early_stopping = True\n",
        "VOCAB_SIZE_WORD = MAX_NUM_WORDS_WORD+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gADH5iyFdQMl"
      },
      "source": [
        "tokenizer1 = Tokenizer(num_words = MAX_NUM_WORDS_WORD+1, oov_token='UNK')\n",
        "tokenizer1.fit_on_texts(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moyQAFqvnAbn"
      },
      "source": [
        "def custom_text_to_word_sequence(texts, modelTokenizer):\n",
        "  vects = []\n",
        "  for text in texts:\n",
        "    seq = text.split(' ')\n",
        "    seq = [i for i in seq if i]\n",
        "\n",
        "    vect = []\n",
        "    for w in seq:\n",
        "      try:\n",
        "        ind = modelTokenizer.word_index[w]\n",
        "        if (ind<=MAX_NUM_WORDS_WORD):\n",
        "          vect.append(ind)\n",
        "        else:\n",
        "          vect.append(modelTokenizer.word_index['UNK'])\n",
        "      except KeyError:\n",
        "        vect.append(modelTokenizer.word_index['UNK'])\n",
        "\n",
        "    vects.append(vect)\n",
        "  \n",
        "  return vects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJWKEBLr1S4M"
      },
      "source": [
        "x_train_tokens1 = custom_text_to_word_sequence(train_text, tokenizer1)\n",
        "x_val_tokens1 = custom_text_to_word_sequence(val_text, tokenizer1)\n",
        "x_test_tokens1 = custom_text_to_word_sequence(test_text, tokenizer1)\n",
        "\n",
        "x_train_pad1 = pad_sequences(x_train_tokens1,maxlen=MAX_SEQUENCE_LENGTH_WORD,padding=\"post\")\n",
        "x_val_pad1 = pad_sequences(x_val_tokens1, maxlen=MAX_SEQUENCE_LENGTH_WORD, padding='post')\n",
        "x_test_pad1 = pad_sequences(x_test_tokens1,maxlen=MAX_SEQUENCE_LENGTH_WORD,padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x21U4jFIeE0O"
      },
      "source": [
        "### Tokenizer for Character Level Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaourxY9eTdQ"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH_CHAR = 1014"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX2CXCU8NB5b"
      },
      "source": [
        "tokenizer2 = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tokenizer2.fit_on_texts(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMwrZe_ecWh"
      },
      "source": [
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "tokenizer2.word_index = char_dict.copy()\n",
        "tokenizer2.word_index[tokenizer2.oov_token] = max(char_dict.values()) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b7qMfpiecdG"
      },
      "source": [
        "x_train_tokens2 = tokenizer2.texts_to_sequences(train_text)\n",
        "x_val_tokens2 = tokenizer2.texts_to_sequences(val_text)\n",
        "x_test_tokens2 = tokenizer2.texts_to_sequences(test_text)\n",
        "\n",
        "x_train_pad2 = pad_sequences(x_train_tokens2,maxlen=MAX_SEQUENCE_LENGTH_CHAR,padding=\"post\")\n",
        "x_val_pad2 = pad_sequences(x_val_tokens2, maxlen=MAX_SEQUENCE_LENGTH_CHAR, padding='post')\n",
        "x_test_pad2 = pad_sequences(x_test_tokens2,maxlen=MAX_SEQUENCE_LENGTH_CHAR,padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmXjQLApXmoD"
      },
      "source": [
        "# import gensim\n",
        "# word2vec_model = gensim.models.Word2Vec(sentences=train_text,size=EMBED_DIM,window=5,workers=4,min_count=1)\n",
        "# vocab_size = MAX_NUM_WORDS+1\n",
        "# embedding_weights = np.zeros((vocab_size,EMBED_DIM))\n",
        "# for word,i in tokenizer.word_index.items():\n",
        "#   if (i<=vocab_size):\n",
        "#     try:\n",
        "#       vector = word2vec_model.wv.get_vector(word)\n",
        "#       embedding_weights[i] = vector\n",
        "#     except KeyError:\n",
        "#       pass\n",
        "#   else:\n",
        "#       break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf8Et2IIgB5S"
      },
      "source": [
        "# Baseline CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnHGkYGY3jUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0908603-220c-4ff2-ed75-0f7823fb82fa"
      },
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH_WORD,), dtype='int32')\n",
        "embedding_layer = Embedding(VOCAB_SIZE_WORD, EMBED_DIM_WORD, input_length=MAX_SEQUENCE_LENGTH_WORD)\n",
        "\n",
        "input_node = Input(shape=(MAX_SEQUENCE_LENGTH_WORD, EMBED_DIM_WORD))\n",
        "conv_list = []\n",
        "\n",
        "conv = Conv1D(filters=10, kernel_size=3, activation='relu')(input_node)\n",
        "drop = Dropout(0.3)(conv)\n",
        "pool = MaxPooling1D(pool_size=2)(drop)\n",
        "flatten = Flatten()(pool)\n",
        "conv_list.append(flatten)\n",
        "\n",
        "conv = Conv1D(filters=10, kernel_size=8, activation='relu')(input_node)\n",
        "drop = Dropout(0.3)(conv)\n",
        "pool = MaxPooling1D(pool_size=2)(drop)\n",
        "flatten = Flatten()(pool)\n",
        "conv_list.append(flatten)\n",
        "\n",
        "out = Concatenate()(conv_list)\n",
        "graph = Model(inputs = input_node, outputs = out)\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(embedding_layer)\n",
        "model1.add(Dropout(0.5, input_shape=(MAX_SEQUENCE_LENGTH_WORD, EMBED_DIM_WORD)))\n",
        "model1.add(graph)\n",
        "model1.add(Dense(50))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=0.0004)\n",
        "\n",
        "model1.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(model1.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 400, 768)          23441664  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 400, 768)          0         \n",
            "                                                                 \n",
            " model (Functional)          (None, 3950)              84500     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                197550    \n",
            "                                                                 \n",
            " activation (Activation)     (None, 50)                0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,723,765\n",
            "Trainable params: 23,723,765\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXs0DTRPkC-K",
        "outputId": "e2062da6-2825-41a7-f9d0-913090ebe70c"
      },
      "source": [
        "tensorboard1 = TensorBoard(log_dir='drive/MyDrive/NNDL_Project/Dataset1/CNN_Baseline_logs/', histogram_freq=0, write_graph=True)\n",
        "early_stopping1 = EarlyStopping(monitor='val_loss', patience = 2, mode = 'min')\n",
        "cp1 = ModelCheckpoint('drive/MyDrive/NNDL_Project/Dataset1/CNN_Baseline_bestModel.h5', monitor='val_acc', save_best_only=True, mode='max')\n",
        "\n",
        "print('using early stopping strategy')\n",
        "history1 = model1.fit(x_train_pad1, train_classes, validation_data=(x_val_pad1, val_classes), epochs=5, batch_size=16, callbacks = [early_stopping1, cp1, tensorboard1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using early stopping strategy\n",
            "Epoch 1/5\n",
            "2188/2188 [==============================] - 119s 40ms/step - loss: 0.3691 - acc: 0.8213 - val_loss: 0.2670 - val_acc: 0.8919\n",
            "Epoch 2/5\n",
            "2188/2188 [==============================] - 84s 39ms/step - loss: 0.1707 - acc: 0.9385 - val_loss: 0.2790 - val_acc: 0.8883\n",
            "Epoch 3/5\n",
            "2188/2188 [==============================] - 87s 40ms/step - loss: 0.0784 - acc: 0.9734 - val_loss: 0.3412 - val_acc: 0.8861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UirpBB9skYwZ",
        "outputId": "fdde76d9-c821-4d43-fdf5-63b46814718e"
      },
      "source": [
        "results1 = model1.evaluate(x_test_pad1, test_classes, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3338 - acc: 0.8865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGjwcrPhvdaM"
      },
      "source": [
        "# %tensorboard --logdir 'drive/MyDrive/NNDL_Project/CNN_Baseline_logs/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0enYqduhZBf"
      },
      "source": [
        "# Character Level CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zGQWS1hhmsT"
      },
      "source": [
        "conv_layers_sm = [[256, 7, 3],\n",
        "                  [256, 7, 3],\n",
        "                  [256, 3, -1],\n",
        "                  [256, 3, -1],\n",
        "                  [256, 3, -1],\n",
        "                  [256, 3, 3]]\n",
        "\n",
        "fully_connected_layers_sm = [1024, 1024]\n",
        "\n",
        "num_of_classes = 2\n",
        "dropout_p = 0.5\n",
        "optimizer = 'adam'\n",
        "loss = 'binary_crossentropy'\n",
        "EMBED_DIM_CHAR = len(tokenizer2.word_index)\n",
        "VOCAB_SIZE_CHAR = EMBED_DIM_CHAR + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj9LKWpMhSiD",
        "outputId": "526232ae-5640-4b47-ebb8-9fc772d52012"
      },
      "source": [
        "# Model Construction\n",
        "# Input\n",
        "inputs = Input(shape=(MAX_SEQUENCE_LENGTH_CHAR,), name='input', dtype='int64')  # shape=(?, 1014)\n",
        "# Embedding\n",
        "x = Embedding(VOCAB_SIZE_CHAR,\n",
        "              EMBED_DIM_CHAR,\n",
        "              input_length=MAX_SEQUENCE_LENGTH_CHAR,)(inputs)\n",
        "# Conv\n",
        "for filter_num, filter_size, pooling_size in conv_layers_sm:\n",
        "    x = Conv1D(filter_num, filter_size)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    if pooling_size != -1:\n",
        "        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
        "x = Flatten()(x)  # (None, 8704)\n",
        "# Fully connected layers\n",
        "for dense_size in fully_connected_layers_sm:\n",
        "    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
        "    x = Dropout(dropout_p)(x)\n",
        "# Output Layer\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "# Build model\n",
        "model2 = Model(inputs=inputs, outputs=predictions)\n",
        "model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, binary_crossentropy\n",
        "print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 1014)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 1014, 69)          4830      \n",
            "                                                                 \n",
            " conv1d_12 (Conv1D)          (None, 1008, 256)         123904    \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 1008, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_6 (MaxPooling  (None, 336, 256)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_13 (Conv1D)          (None, 330, 256)          459008    \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 330, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPooling  (None, 110, 256)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_14 (Conv1D)          (None, 108, 256)          196864    \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 108, 256)          0         \n",
            "                                                                 \n",
            " conv1d_15 (Conv1D)          (None, 106, 256)          196864    \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 106, 256)          0         \n",
            "                                                                 \n",
            " conv1d_16 (Conv1D)          (None, 104, 256)          196864    \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 104, 256)          0         \n",
            "                                                                 \n",
            " conv1d_17 (Conv1D)          (None, 102, 256)          196864    \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 102, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPooling  (None, 34, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 8704)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1024)              8913920   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,339,743\n",
            "Trainable params: 11,339,743\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ROHoidnbax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383cc877-01fb-4cd1-8162-fd355c8f462a"
      },
      "source": [
        "tensorboard2 = TensorBoard(log_dir='drive/MyDrive/NNDL_Project/Dataset1/CNN_Char_sm_logs/', histogram_freq=0, write_graph=True)\n",
        "early_stopping2 = EarlyStopping(monitor='val_loss', patience = 2, mode = 'min')\n",
        "cp2 = ModelCheckpoint('drive/MyDrive/NNDL_Project/Dataset1/CNN_Char_sm_bestModel.h5', monitor='val_acc', save_best_only=True, mode='max')\n",
        "\n",
        "print('using early stopping strategy')\n",
        "history2 = model2.fit(x_train_pad2, train_classes, validation_data=(x_val_pad2, val_classes), epochs=5, batch_size=16, callbacks = [early_stopping2, cp2, tensorboard2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using early stopping strategy\n",
            "Epoch 1/5\n",
            "2188/2188 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5020WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "2188/2188 [==============================] - 98s 44ms/step - loss: 0.6936 - accuracy: 0.5020 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "2188/2188 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5016WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "2188/2188 [==============================] - 100s 46ms/step - loss: 0.6933 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 3/5\n",
            "2188/2188 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4999WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "2188/2188 [==============================] - 97s 44ms/step - loss: 0.6934 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "2188/2188 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "2188/2188 [==============================] - 101s 46ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 5/5\n",
            "2188/2188 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5006WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "2188/2188 [==============================] - 101s 46ms/step - loss: 0.6932 - accuracy: 0.5006 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qnYVc_9pOJ4",
        "outputId": "9a4f6249-9118-4e15-cfb9-52aaa9b7fd6e"
      },
      "source": [
        "results2 = model2.evaluate(x_test_pad2, test_classes, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "469/469 [==============================] - 7s 15ms/step - loss: 0.6932 - accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwpCmf8Jnu37"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
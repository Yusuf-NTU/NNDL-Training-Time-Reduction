{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-XeR8Sbz0B5x"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dannu15yD_tu"
   },
   "source": [
    "##Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wJ629yR6I4if"
   },
   "outputs": [],
   "source": [
    "def updating_interval(total_iterations, number_of_desired_updates):\n",
    "    '''\n",
    "    This function will try to pick an intelligent progress update interval \n",
    "    based on the magnitude of the total iterations.\n",
    "\n",
    "    Parameters:\n",
    "      `total_iters` - The number of iterations in the for-loop.\n",
    "      `num_desired_updates` - How many times we want to see an update over the \n",
    "                              course of the for-loop.\n",
    "    '''\n",
    "    \n",
    "    # Divide the total iterations by the desired number of updates.\n",
    "    exact_interval = total_iterations / number_of_desired_updates\n",
    "\n",
    "    # The `round` function has the ability to round down a number to, e.g., the\n",
    "    # nearest thousandth: round(exact_interval, -3)\n",
    "    #\n",
    "    # To determine the magnitude to round to, find the magnitude of the total,\n",
    "    # and then go one magnitude below that.\n",
    "\n",
    "    # Get the order of magnitude of the total.\n",
    "    order_of_magntiude = len(str(total_iterations)) - 1\n",
    "\n",
    "    # Our update interval should be rounded to an order of magnitude smaller. \n",
    "    round_magnitude = order_of_magntiude - 1\n",
    "\n",
    "    # Round down and cast to an int.\n",
    "    update_interval = int(round(exact_interval, -round_magnitude))\n",
    "\n",
    "    # Don't allow the interval to be zero!\n",
    "    if update_interval == 0:\n",
    "        update_interval = 1\n",
    "\n",
    "    return update_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gpt6tR83keZD"
   },
   "outputs": [],
   "source": [
    "def show_time(elapsed_time):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_time_rounded = int(round((elapsed_time)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_time_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6PCbHwTKE976"
   },
   "outputs": [],
   "source": [
    "def extract_data(data):\n",
    "    '''\n",
    "    Extract text and labels in a list\n",
    "    '''\n",
    "    \n",
    "    text = []\n",
    "    labels=[]\n",
    "    \n",
    "    # Iterate over the data\n",
    "    for idx, row in data.iterrows():\n",
    "        # The text is a `bytes` object, decode to string.\n",
    "        text.append(row['clean_text'])\n",
    "\n",
    "        # Cast the label from `np.int64` to `int`\n",
    "        labels.append(int(row['labels']))\n",
    "        \n",
    "    return text, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cairkv4V7_IE"
   },
   "source": [
    "##Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7N6YR1RoFJe_",
    "outputId": "571162da-1416-4573-a616-8feb571c7272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jDhtyywoFJR2"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/content/drive/MyDrive/NNDL_Datasets/IMDB_train.csv')\n",
    "test_data = pd.read_csv('/content/drive/MyDrive/NNDL_Datasets/IMDB_test.csv')\n",
    "val_data = pd.read_csv('/content/drive/MyDrive/NNDL_Datasets/IMDB_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "gx3aeVLJBuKC"
   },
   "outputs": [],
   "source": [
    "# For YELP data\n",
    "#train_data = pd.read_csv('/content/drive/MyDrive/NNDL_Datasets/YELP_train.csv')\n",
    "#test_data = pd.read_csv('/content/drive/MyDrive/NNDL_Datasets/YELP_test.csv')\n",
    "#val_data = pd.read_csv('/content/drive/MyDrive/NNDL_Datasets/YELP_val.csv')\n",
    "#train_data['labels'] = train_data['labels']-1\n",
    "#test_data['labels'] = test_data['labels']-1\n",
    "#val_data['labels'] = val_data['labels']-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg9rRznq42f8"
   },
   "source": [
    "## Extract data and see the shape and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ2_hJjzDYur",
    "outputId": "28293a3d-b818-40a2-da79-8639eadda9b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35,000 Training Samples\n",
      "7,500 Test Samples\n",
      "7,500 Validation Samples\n",
      "Labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "train_text, train_labels = extract_data(train_data)\n",
    "test_text, test_labels = extract_data(test_data)\n",
    "val_text, val_labels = extract_data(val_data)\n",
    "\n",
    "# Print some stats.\n",
    "print('{:,} Training Samples'.format(len(train_labels)))\n",
    "print('{:,} Test Samples'.format(len(test_labels)))\n",
    "print('{:,} Validation Samples'.format(len(val_labels)))\n",
    "print('Labels:', np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gObslUje8oNy"
   },
   "source": [
    "## See few examples of review and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Njp6-5b5NwiE",
    "outputId": "8f8b94d1-0d1d-48e9-f8e2-57a89a38e437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: john garfield plays marine blinded grenade fighting guadalcanal learn live disability stereotypical notions blindness sure hell burden everyone hospital staff fellow wounded marines cant get neither girl back home played eleanor parker hes stubborn blinded fears self pity prejudices complex role garfield carries memorably great performance keeps one watching spite ever present syrupy melodrama best scenes guadalcanal hes machine gun nest trying fend advancing japanese soldiers hellish looking night time battle later dream sequence hospital sees walking train platform white cane dark glasses holding tin cup girlfriend walks backward away camera\n",
      "Label: 1\n",
      "-----------------\n",
      "Review: saw anatomy years ago dubbed friends house dont remember much saw video store second one really related first one franka ponte makes little cameo one okay good first one im seeing first one tonight dubbed collectors edition really like german movies like one interesting people cults like one movie could exist think dunno grossly entertaining scary anatomy 2 little different characters good first really thought anatomy interesting good see second one\n",
      "Label: 1\n",
      "-----------------\n",
      "Review: smallville episode justice best episode smallville favorite episode smallville\n",
      "Label: 1\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(train_text[:3], train_labels[:3]):\n",
    "    print('Review:', i)\n",
    "    print('Label:', j)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDfRG57sfEjX"
   },
   "source": [
    "## Check class distrubution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "fcBinIxWLjer",
    "outputId": "5eb223ed-9e5f-4e5d-c3f4-5e153b1668d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "\n",
    "# Plot the number of tokens of each length.\n",
    "ax = sns.countplot(train_labels)\n",
    "\n",
    "# Add labels\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Number of Training Samples')\n",
    "\n",
    "# Add thousands separators to the y-axis labels.\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydHIQ_yc9ifE"
   },
   "source": [
    "#Smart Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vuahmt8IGHvG"
   },
   "source": [
    "##Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z474sSC6oe7A",
    "outputId": "77c34e79-9221-40af-f032-c89a6c030d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "print('Loading tokenizer...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "rVmEq4n7F1um"
   },
   "outputs": [],
   "source": [
    "# For BERT model\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "etRvZ5HRF1FF"
   },
   "outputs": [],
   "source": [
    "# For DistilBERT model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaIlkeK2eDJw"
   },
   "source": [
    "##Fixed Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iC1gu9xU5RmR",
    "outputId": "2f4e8986-f0ee-4e58-d887-da90e8b1e1fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 35,000 training samples\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 2,400 samples.\n",
      "  Tokenized 4,800 samples.\n",
      "  Tokenized 7,200 samples.\n",
      "  Tokenized 9,600 samples.\n",
      "  Tokenized 12,000 samples.\n",
      "  Tokenized 14,400 samples.\n",
      "  Tokenized 16,800 samples.\n",
      "  Tokenized 19,200 samples.\n",
      "  Tokenized 21,600 samples.\n",
      "  Tokenized 24,000 samples.\n",
      "  Tokenized 26,400 samples.\n",
      "  Tokenized 28,800 samples.\n",
      "  Tokenized 31,200 samples.\n",
      "  Tokenized 33,600 samples.\n"
     ]
    }
   ],
   "source": [
    "fixed_padding = True\n",
    "\n",
    "if fixed_padding:\n",
    "\n",
    "    # Specify batch_size and truncation length.    \n",
    "    batch_size = 16\n",
    "    max_len = 400   \n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} training samples'.format(len(train_text)))\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    batches_of_input_ids = []\n",
    "    batches_of_attention_masks = []\n",
    "    batches_of_labels = []\n",
    "\n",
    "    update_interval = batch_size * 150 \n",
    "\n",
    "    # For every sequence\n",
    "    for i in range(0, len(train_text), batch_size):\n",
    "\n",
    "        if ((i % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(i))\n",
    "\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.batch_encode_plus(\n",
    "                            train_text[i:i+batch_size],     # Batch of sentences to encode.\n",
    "                            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 400,               # Pad & truncate all sentences.\n",
    "                            padding = 'max_length',         # Pad all to the `max_length` parameter.\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',          # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        batches_of_input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        batches_of_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # Add the labels for the batch\n",
    "        batches_of_labels.append(torch.tensor(train_labels[i:i+batch_size]))\n",
    "    \n",
    "    # Rename the final variable to match the rest of the code in this Notebook.\n",
    "    py_inputs = batches_of_input_ids\n",
    "    py_attn_masks = batches_of_attention_masks\n",
    "    py_labels = batches_of_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvuYLP7TDpXH"
   },
   "source": [
    "#Fine-tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwdfO8VuxHE-"
   },
   "source": [
    "##Load Configuration and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "8GZPd5vWKrpL"
   },
   "outputs": [],
   "source": [
    "# For BERT model\n",
    "\n",
    "# Load the Config object, with an output configured for classification.\n",
    "#config = AutoConfig.from_pretrained(pretrained_model_name_or_path='bert-base-uncased',\n",
    "                                    #num_labels=2)\n",
    "\n",
    "# Load the pre-trained model for classification, passing in the `config` from above.\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path='bert-base-uncased',\n",
    "                                                           #config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "d6ec7d1ac94947f9b8678032d5106b71",
      "05319887ec6c4e3ca0512f1ea0163cc1",
      "f61d973102ae48a7957113339c2f9e11",
      "0e75fabde26440df803dd90a49aa4059",
      "e6fdcc0a00984f6cb401990c163dbbb5",
      "fa6d44de6beb4c42972262301e1a8d95",
      "3ccc69e78bb44661b4b366160fc36d01",
      "d03b18f7a93f4c7a9effe4ab04b7c961",
      "ea7b5b6109b342a2a74cd7cea8c0090e",
      "1e57098e687a4b71b6e7459d74e124b6",
      "67326933ea5d4962bf6c040100b33a69"
     ]
    },
    "id": "EyWp2fwXHxXT",
    "outputId": "d62ddf4f-672e-4459-b354-3d31f1503f9f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ec7d1ac94947f9b8678032d5106b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# For DistilBERT model\n",
    "\n",
    "# Load the Config object, with an output configured for classification.\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name_or_path='distilbert-base-uncased',\n",
    "                                    num_labels=2)\n",
    "\n",
    "# Load the pre-trained model for classification, passing in the `config` from above.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path='distilbert-base-uncased',\n",
    "                                                           config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBkOcKpxhicl",
    "outputId": "1c8289ee-8e16-4470-91e3-76b768933cc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2831"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect garbage\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Asqmh0uTKuTx",
    "outputId": "9f9b33c1-3625-4113-b3cb-7399f6decae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model to GPU\n",
      "--- GPU: Tesla K80\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Loading model to GPU')\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print('--- GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "desc = model.to(device)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRWT-D4U_Pvx"
   },
   "source": [
    "## Setup the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "GLs72DuMODJO"
   },
   "outputs": [],
   "source": [
    "# AdamW is a class from the huggingface library, 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # Got this value after hyper parameter optimization\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_sAE_K9IHM_"
   },
   "source": [
    "## Set up the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-p0upAhhRiIx"
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# Note that it's the number of *batches*, not *samples*!\n",
    "total_steps = len(py_inputs) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqfmWwUR_Sox"
   },
   "source": [
    "##Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6J-FYdx6nFE_",
    "outputId": "8beae38d-61b2-42a4-80d5-6949da78c835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training on 2,188 batches\n",
      "  Batch     200  of    2,188.    Elapsed: 0:03:48.\n",
      "  Batch     400  of    2,188.    Elapsed: 0:07:36.\n",
      "  Batch     600  of    2,188.    Elapsed: 0:11:24.\n",
      "  Batch     800  of    2,188.    Elapsed: 0:15:12.\n",
      "  Batch   1,000  of    2,188.    Elapsed: 0:19:01.\n",
      "  Batch   1,200  of    2,188.    Elapsed: 0:22:49.\n",
      "  Batch   1,400  of    2,188.    Elapsed: 0:26:37.\n",
      "  Batch   1,600  of    2,188.    Elapsed: 0:30:25.\n",
      "  Batch   1,800  of    2,188.    Elapsed: 0:34:13.\n",
      "  Batch   2,000  of    2,188.    Elapsed: 0:38:02.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epcoh took: 0:41:36\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:41:36 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# This training code is based on the `run_glue.py` script of huggingface.\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 321\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Update every `update_interval` batches.\n",
    "update_interval = updating_interval(total_iterations=len(py_inputs), number_of_desired_updates=10)\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "epochs=1\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    \n",
    "    # At the start of each epoch (except for the first) we need to re-randomize our training data.\n",
    "    if epoch_i > 0:\n",
    "        # Use our `make_smart_batches` function to re-shuffle the dataset into new batches.\n",
    "        (py_inputs, py_attn_masks, py_labels) = make_smart_batches(train_text, train_labels, batch_size)\n",
    "    \n",
    "    print('Training on {:,} batches'.format(len(py_inputs)))\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data.\n",
    "    for step in range(0, len(py_inputs)):\n",
    "\n",
    "        # Progress update every, e.g., 100 batches.\n",
    "        if step % update_interval == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = show_time(time.time() - t0)\n",
    "            \n",
    "            # Calculate the time remaining based on our progress.\n",
    "            steps_per_sec = (time.time() - t0) / step\n",
    "            remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.'.format(step, len(py_inputs), elapsed))\n",
    "\n",
    "        # Copy the current training batch to the GPU using the `to` method.\n",
    "        b_input_ids = py_inputs[step].to(device)\n",
    "        b_input_mask = py_attn_masks[step].to(device)\n",
    "        b_labels = py_labels[step].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The call returns the loss (because we provided labels) and the \n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels).values()\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(py_inputs)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = show_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(show_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5eZFCVPPn7J"
   },
   "source": [
    "##Evaluating the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beKYmu9QNGhY",
    "outputId": "c321c74a-bd42-49af-9691-957788f90979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 7,500 testing samples\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 2,400 samples.\n",
      "  Tokenized 4,800 samples.\n",
      "  Tokenized 7,200 samples.\n",
      "Predicting labels for 7,500 test sentences\n",
      "  Batch      50  of      469.    Elapsed: 0:00:21.\n",
      "  Batch     100  of      469.    Elapsed: 0:00:43.\n",
      "  Batch     150  of      469.    Elapsed: 0:01:04.\n",
      "  Batch     200  of      469.    Elapsed: 0:01:25.\n",
      "  Batch     250  of      469.    Elapsed: 0:01:46.\n",
      "  Batch     300  of      469.    Elapsed: 0:02:07.\n",
      "  Batch     350  of      469.    Elapsed: 0:02:29.\n",
      "  Batch     400  of      469.    Elapsed: 0:02:50.\n",
      "  Batch     450  of      469.    Elapsed: 0:03:11.\n",
      "DONE.\n",
      "Accuracy: 0.912\n"
     ]
    }
   ],
   "source": [
    "fixed_padding = True\n",
    "\n",
    "if fixed_padding:\n",
    "\n",
    "    # Specify batch_size and truncation length.    \n",
    "    batch_size = 16\n",
    "    max_len = 400   \n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} testing samples'.format(len(test_text)))\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    batches_of_input_ids = []\n",
    "    batches_of_attention_masks = []\n",
    "    batches_of_labels = []\n",
    "\n",
    "    update_interval = batch_size * 150 \n",
    "\n",
    "    # For every sequence\n",
    "    for i in range(0, len(test_text), batch_size):\n",
    "\n",
    "        if ((i % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(i))\n",
    "\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.batch_encode_plus(\n",
    "                            test_text[i:i+batch_size],     # Batch of sentences to encode.\n",
    "                            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 400,               # Pad & truncate all sentences.\n",
    "                            padding = 'max_length',         # Pad all to the `max_length` parameter.\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',          # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        batches_of_input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        batches_of_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # Add the labels for the batch\n",
    "        batches_of_labels.append(torch.tensor(test_labels[i:i+batch_size]))\n",
    "    \n",
    "    # Rename the final variable to match the rest of the code in this Notebook.\n",
    "    py_inputs = batches_of_input_ids\n",
    "    py_attn_masks = batches_of_attention_masks\n",
    "    py_labels = batches_of_labels\n",
    "\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences'.format(len(test_labels)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Choose an interval on which to print progress updates.\n",
    "update_interval = updating_interval(total_iterations=len(py_inputs), number_of_desired_updates=10)\n",
    "\n",
    "# Measure elapsed time.\n",
    "t0 = time.time()\n",
    "\n",
    "# For each batch of the data\n",
    "for step in range(0, len(py_inputs)):\n",
    "\n",
    "    # Progress update every 100 batches.\n",
    "    if step % update_interval == 0 and not step == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = show_time(time.time() - t0)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.'.format(step, len(py_inputs), elapsed))\n",
    "\n",
    "    # Copy the batch to the GPU.\n",
    "    b_input_ids = py_inputs[step].to(device)\n",
    "    b_input_mask = py_attn_masks[step].to(device)\n",
    "    b_labels = py_labels[step].to(device)\n",
    "  \n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('DONE.')\n",
    "\n",
    "# Combine the results across the batches.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Choose the label with the highest score as our prediction.\n",
    "preds = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Calculate simple flat accuracy -- number correct over total number.\n",
    "accuracy = (preds == true_labels).mean()\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RfSDldsUmKn"
   },
   "source": [
    "##Evaluating the model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hba10sXR7Xi6",
    "outputId": "d431f954-bade-4e33-cb43-8263e03dae05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 7,500 testing samples\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 2,400 samples.\n",
      "  Tokenized 4,800 samples.\n",
      "  Tokenized 7,200 samples.\n",
      "Predicting labels for 7,500 test sentences\n",
      "  Batch      50  of      469.    Elapsed: 0:00:21.\n",
      "  Batch     100  of      469.    Elapsed: 0:00:42.\n",
      "  Batch     150  of      469.    Elapsed: 0:01:04.\n",
      "  Batch     200  of      469.    Elapsed: 0:01:25.\n",
      "  Batch     250  of      469.    Elapsed: 0:01:46.\n",
      "  Batch     300  of      469.    Elapsed: 0:02:07.\n",
      "  Batch     350  of      469.    Elapsed: 0:02:29.\n",
      "  Batch     400  of      469.    Elapsed: 0:02:50.\n",
      "  Batch     450  of      469.    Elapsed: 0:03:11.\n",
      "DONE.\n",
      "Accuracy: 0.910\n"
     ]
    }
   ],
   "source": [
    "fixed_padding = True\n",
    "\n",
    "if fixed_padding:\n",
    "\n",
    "    # Specify batch_size and truncation length.    \n",
    "    batch_size = 16\n",
    "    max_len = 400   \n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} testing samples'.format(len(val_text)))\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    batches_of_input_ids = []\n",
    "    batches_of_attention_masks = []\n",
    "    batches_of_labels = []\n",
    "\n",
    "    update_interval = batch_size * 150 \n",
    "\n",
    "    # For every sequence\n",
    "    for i in range(0, len(val_text), batch_size):\n",
    "\n",
    "        if ((i % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(i))\n",
    "\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.batch_encode_plus(\n",
    "                            val_text[i:i+batch_size],     # Batch of sentences to encode.\n",
    "                            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 400,               # Pad & truncate all sentences.\n",
    "                            padding = 'max_length',         # Pad all to the `max_length` parameter.\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',          # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        batches_of_input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        batches_of_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # Add the labels for the batch\n",
    "        batches_of_labels.append(torch.tensor(val_labels[i:i+batch_size]))\n",
    "    \n",
    "    # Rename the final variable to match the rest of the code in this Notebook.\n",
    "    py_inputs = batches_of_input_ids\n",
    "    py_attn_masks = batches_of_attention_masks\n",
    "    py_labels = batches_of_labels\n",
    "\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences'.format(len(val_labels)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Choose an interval on which to print progress updates.\n",
    "update_interval = updating_interval(total_iterations=len(py_inputs), number_of_desired_updates=10)\n",
    "\n",
    "# Measure elapsed time.\n",
    "t0 = time.time()\n",
    "\n",
    "# For each batch of the data\n",
    "for step in range(0, len(py_inputs)):\n",
    "\n",
    "    # Progress update every 100 batches.\n",
    "    if step % update_interval == 0 and not step == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = show_time(time.time() - t0)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.'.format(step, len(py_inputs), elapsed))\n",
    "\n",
    "    # Copy the batch to the GPU.\n",
    "    b_input_ids = py_inputs[step].to(device)\n",
    "    b_input_mask = py_attn_masks[step].to(device)\n",
    "    b_labels = py_labels[step].to(device)\n",
    "  \n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('DONE.')\n",
    "\n",
    "# Combine the results across the batches.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Choose the label with the highest score as our prediction.\n",
    "preds = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Calculate simple flat accuracy -- number correct over total number.\n",
    "accuracy = (preds == true_labels).mean()\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCqhHHtXKIyV"
   },
   "source": [
    "## Count Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hv8pz2ZpSODC",
    "outputId": "d1aaf6e4-0b87-4a20-9a60-ec6128ce85f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66955010\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformer Standard Approach",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05319887ec6c4e3ca0512f1ea0163cc1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e75fabde26440df803dd90a49aa4059": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea7b5b6109b342a2a74cd7cea8c0090e",
      "max": 267967963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d03b18f7a93f4c7a9effe4ab04b7c961",
      "value": 267967963
     }
    },
    "1e57098e687a4b71b6e7459d74e124b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ccc69e78bb44661b4b366160fc36d01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67326933ea5d4962bf6c040100b33a69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d03b18f7a93f4c7a9effe4ab04b7c961": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6ec7d1ac94947f9b8678032d5106b71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f61d973102ae48a7957113339c2f9e11",
       "IPY_MODEL_0e75fabde26440df803dd90a49aa4059",
       "IPY_MODEL_e6fdcc0a00984f6cb401990c163dbbb5"
      ],
      "layout": "IPY_MODEL_05319887ec6c4e3ca0512f1ea0163cc1"
     }
    },
    "e6fdcc0a00984f6cb401990c163dbbb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67326933ea5d4962bf6c040100b33a69",
      "placeholder": "​",
      "style": "IPY_MODEL_1e57098e687a4b71b6e7459d74e124b6",
      "value": " 256M/256M [00:08&lt;00:00, 34.3MB/s]"
     }
    },
    "ea7b5b6109b342a2a74cd7cea8c0090e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f61d973102ae48a7957113339c2f9e11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ccc69e78bb44661b4b366160fc36d01",
      "placeholder": "​",
      "style": "IPY_MODEL_fa6d44de6beb4c42972262301e1a8d95",
      "value": "Downloading: 100%"
     }
    },
    "fa6d44de6beb4c42972262301e1a8d95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

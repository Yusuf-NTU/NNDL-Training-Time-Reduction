{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNDL_Dataset_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j--gbGjBFCGl",
        "N3MDdCItHGhb",
        "CKUG0AonHiY7",
        "aPLAv_O5HvdK",
        "eoXf0iM-JMI5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "idct0jSO4He1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense,Embedding,LSTM,GRU,Flatten,Dropout, Input, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
        "from keras.layers import Concatenate, Activation\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAhJdD7f5hei",
        "outputId": "b77ad58f-141b-46e9-d3ce-0d4a5d5767f2"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxiPvh516Sci"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRX4gb_-8vC-"
      },
      "source": [
        "# df_train_full = pd.read_csv(\"drive/MyDrive/NNDL_Project/train_YELP.csv\", header=None)\n",
        "# df_test_full = pd.read_csv(\"drive/MyDrive/NNDL_Project/test_YELP.csv\", header=None)\n",
        "# df_train_full.columns = ['label', 'review']\n",
        "# df_test_full.columns = ['label', 'review']\n",
        "# df_full = pd.concat([df_train_full.sample(127500), df_test_full.sample(22500)])\n",
        "# df_full.index = np.arange(len(df_full))\n",
        "\n",
        "# X = df_full['review']\n",
        "# y = df_full['label']\n",
        "\n",
        "# xtr , xte , ytr, yte = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)\n",
        "# xte, xval, yte, yval = train_test_split(xte, yte, test_size=0.5, random_state=1, stratify=yte)\n",
        "\n",
        "# train = pd.DataFrame([xtr, ytr]).T\n",
        "# train.columns = ['review', 'label']\n",
        "# train.index = np.arange(len(train))\n",
        "\n",
        "# val = pd.DataFrame([xval, yval]).T\n",
        "# val.columns = ['review', 'label']\n",
        "# val.index = np.arange(len(val))\n",
        "\n",
        "# test = pd.DataFrame([xte, yte]).T\n",
        "# test.columns = ['review', 'label']\n",
        "# test.index = np.arange(len(test))\n",
        "\n",
        "# train.to_csv('drive/MyDrive/NNDL_Project/YELP_TRAIN_105K.csv', index=False)\n",
        "# val.to_csv('drive/MyDrive/NNDL_Project/YELP_VAL_22.5K.csv', index=False)\n",
        "# test.to_csv('drive/MyDrive/NNDL_Project/YELP_TEST_22.5K.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYULI8-CCmoZ"
      },
      "source": [
        "df_train = pd.read_csv(\"drive/MyDrive/NNDL_Datasets/YELP_train.csv\")\n",
        "df_val = pd.read_csv(\"drive/MyDrive/NNDL_Datasets/YELP_val.csv\")\n",
        "df_test = pd.read_csv(\"drive/MyDrive/NNDL_Datasets/YELP_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-n1X6FlNMxu",
        "outputId": "8967c20b-fd49-4710-b55b-39ab51c6ef5a"
      },
      "source": [
        "len(df_train), len(df_val), len(df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105000, 22500, 22500)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0EFSgHO5-Wu"
      },
      "source": [
        "# df_train.columns = ['review', 'label']\n",
        "# df_val.columns = ['review', 'label']\n",
        "# df_test.columns = ['review', 'label']\n",
        "\n",
        "# df_train.to_csv('drive/MyDrive/NNDL_Datasets/YELP_train.csv', index=False)\n",
        "# df_val.to_csv('drive/MyDrive/NNDL_Datasets/YELP_val.csv', index=False)\n",
        "# df_test.to_csv('drive/MyDrive/NNDL_Datasets/YELP_test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j--gbGjBFCGl"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlMRGaa88m_q"
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def text_preprocessing(dataframe):\n",
        "\n",
        "  lines = dataframe[\"review\"].values.tolist()\n",
        "  reviews = list()\n",
        "\n",
        "  for line in lines:\n",
        "      tokens = word_tokenize(line)\n",
        "      tokens = [w.lower() for w in tokens]\n",
        "      table = str.maketrans(\"\",\"\",string.punctuation)\n",
        "      stripped = [w.translate(table) for w in tokens]\n",
        "      words = [w for w in stripped if w.isalpha()]\n",
        "      words = [w for w in words if w not in stop_words]\n",
        "      reviews.append(words)\n",
        "\n",
        "  reviews = [' '.join(review) for review in reviews]\n",
        "  dataframe['prepro_rev'] = reviews\n",
        "  return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znkrxqx_FLYU"
      },
      "source": [
        "df_train = text_preprocessing(df_train)\n",
        "df_val = text_preprocessing(df_val)\n",
        "df_test = text_preprocessing(df_test)\n",
        "\n",
        "train_text = df_train['prepro_rev'].values\n",
        "val_text = df_val['prepro_rev'].values\n",
        "test_text = df_test['prepro_rev'].values\n",
        "\n",
        "train_text = [s.lower() for s in train_text]\n",
        "val_text = [s.lower() for s in val_text]\n",
        "test_text = [s.lower() for s in test_text]\n",
        "\n",
        "train_classes = df_train['label'].values - 1\n",
        "val_classes = df_val['label'].values - 1\n",
        "test_classes = df_test['label'].values - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3MDdCItHGhb"
      },
      "source": [
        "### Tokenizer for Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv5S2gdJHJYB"
      },
      "source": [
        "EMBED_DIM_WORD = 768\n",
        "MAX_NUM_WORDS_WORD = 30522\n",
        "MAX_SEQUENCE_LENGTH_WORD = 400\n",
        "do_early_stopping = True\n",
        "VOCAB_SIZE_WORD = MAX_NUM_WORDS_WORD+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irPnqxQbFn6D"
      },
      "source": [
        "tokenizer1 = Tokenizer(num_words = MAX_NUM_WORDS_WORD+1, oov_token='UNK')\n",
        "tokenizer1.fit_on_texts(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_4OPo2fHPrt"
      },
      "source": [
        "def custom_text_to_word_sequence(texts, modelTokenizer):\n",
        "  vects = []\n",
        "  for text in texts:\n",
        "    seq = text.split(' ')\n",
        "    seq = [i for i in seq if i]\n",
        "\n",
        "    vect = []\n",
        "    for w in seq:\n",
        "      try:\n",
        "        ind = modelTokenizer.word_index[w]\n",
        "        if (ind<=MAX_NUM_WORDS_WORD):\n",
        "          vect.append(ind)\n",
        "        else:\n",
        "          vect.append(modelTokenizer.word_index['UNK'])\n",
        "      except KeyError:\n",
        "        vect.append(modelTokenizer.word_index['UNK'])\n",
        "\n",
        "    vects.append(vect)\n",
        "  \n",
        "  return vects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UQXlgiuHRKm"
      },
      "source": [
        "x_train_tokens1 = custom_text_to_word_sequence(train_text, tokenizer1)\n",
        "x_val_tokens1 = custom_text_to_word_sequence(val_text, tokenizer1)\n",
        "x_test_tokens1 = custom_text_to_word_sequence(test_text, tokenizer1)\n",
        "\n",
        "x_train_pad1 = pad_sequences(x_train_tokens1,maxlen=MAX_SEQUENCE_LENGTH_WORD,padding=\"post\")\n",
        "x_val_pad1 = pad_sequences(x_val_tokens1, maxlen=MAX_SEQUENCE_LENGTH_WORD, padding='post')\n",
        "x_test_pad1 = pad_sequences(x_test_tokens1,maxlen=MAX_SEQUENCE_LENGTH_WORD,padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKUG0AonHiY7"
      },
      "source": [
        "### Tokenizer for Character Level Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INbWJUJZHSaz"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH_CHAR = 1014"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luB6Am-pHW-8"
      },
      "source": [
        "tokenizer2 = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tokenizer2.fit_on_texts(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrbwQtCMHnRl"
      },
      "source": [
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "tokenizer2.word_index = char_dict.copy()\n",
        "tokenizer2.word_index[tokenizer2.oov_token] = max(char_dict.values()) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyhyYb4mHpkR"
      },
      "source": [
        "x_train_tokens2 = tokenizer2.texts_to_sequences(train_text)\n",
        "x_val_tokens2 = tokenizer2.texts_to_sequences(val_text)\n",
        "x_test_tokens2 = tokenizer2.texts_to_sequences(test_text)\n",
        "\n",
        "x_train_pad2 = pad_sequences(x_train_tokens2,maxlen=MAX_SEQUENCE_LENGTH_CHAR,padding=\"post\")\n",
        "x_val_pad2 = pad_sequences(x_val_tokens2, maxlen=MAX_SEQUENCE_LENGTH_CHAR, padding='post')\n",
        "x_test_pad2 = pad_sequences(x_test_tokens2,maxlen=MAX_SEQUENCE_LENGTH_CHAR,padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPLAv_O5HvdK"
      },
      "source": [
        "# Baseline CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w95iUuHRHqxj",
        "outputId": "7bd30559-d771-4bd2-92ac-c749e2048953"
      },
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH_WORD,), dtype='int32')\n",
        "embedding_layer = Embedding(VOCAB_SIZE_WORD, EMBED_DIM_WORD, input_length=MAX_SEQUENCE_LENGTH_WORD)\n",
        "\n",
        "input_node = Input(shape=(MAX_SEQUENCE_LENGTH_WORD, EMBED_DIM_WORD))\n",
        "conv_list = []\n",
        "\n",
        "conv = Conv1D(filters=10, kernel_size=3, activation='relu')(input_node)\n",
        "drop = Dropout(0.3)(conv)\n",
        "pool = MaxPooling1D(pool_size=2)(drop)\n",
        "flatten = Flatten()(pool)\n",
        "conv_list.append(flatten)\n",
        "\n",
        "conv = Conv1D(filters=10, kernel_size=8, activation='relu')(input_node)\n",
        "drop = Dropout(0.3)(conv)\n",
        "pool = MaxPooling1D(pool_size=2)(drop)\n",
        "flatten = Flatten()(pool)\n",
        "conv_list.append(flatten)\n",
        "\n",
        "out = Concatenate()(conv_list)\n",
        "graph = Model(inputs = input_node, outputs = out)\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(embedding_layer)\n",
        "model1.add(Dropout(0.5, input_shape=(MAX_SEQUENCE_LENGTH_WORD, EMBED_DIM_WORD)))\n",
        "model1.add(graph)\n",
        "model1.add(Dense(50))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=0.0004)\n",
        "\n",
        "model1.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(model1.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 400, 768)          23441664  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 400, 768)          0         \n",
            "                                                                 \n",
            " model (Functional)          (None, 3950)              84500     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                197550    \n",
            "                                                                 \n",
            " activation (Activation)     (None, 50)                0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,723,765\n",
            "Trainable params: 23,723,765\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbQ5oTKvHzX8",
        "outputId": "38ced820-892b-4be0-e2a2-be5e8996e070"
      },
      "source": [
        "tensorboard1 = TensorBoard(log_dir='drive/MyDrive/NNDL_Project/Dataset2/CNN_Baseline_logs/', histogram_freq=0, write_graph=True)\n",
        "early_stopping1 = EarlyStopping(monitor='val_loss', patience = 2, mode = 'min')\n",
        "cp1 = ModelCheckpoint('drive/MyDrive/NNDL_Project/Dataset2/CNN_Baseline_bestModel.h5', monitor='val_acc', save_best_only=True, mode='max')\n",
        "\n",
        "print('using early stopping strategy')\n",
        "history1 = model1.fit(x_train_pad1, train_classes, validation_data=(x_val_pad1, val_classes), epochs=5, batch_size=16, callbacks = [early_stopping1, cp1, tensorboard1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using early stopping strategy\n",
            "Epoch 1/5\n",
            "6563/6563 [==============================] - 280s 38ms/step - loss: 0.2486 - acc: 0.8948 - val_loss: 0.1950 - val_acc: 0.9251\n",
            "Epoch 2/5\n",
            "6563/6563 [==============================] - 251s 38ms/step - loss: 0.1573 - acc: 0.9407 - val_loss: 0.1876 - val_acc: 0.9268\n",
            "Epoch 3/5\n",
            "6563/6563 [==============================] - 245s 37ms/step - loss: 0.1153 - acc: 0.9580 - val_loss: 0.2005 - val_acc: 0.9251\n",
            "Epoch 4/5\n",
            "6563/6563 [==============================] - 245s 37ms/step - loss: 0.0791 - acc: 0.9718 - val_loss: 0.2284 - val_acc: 0.9190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktrGg--pJdnn",
        "outputId": "cd1588ea-a5fe-4b27-f383-1af55ac9254a"
      },
      "source": [
        "results1 = model1.evaluate(x_test_pad1, test_classes, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1407/1407 [==============================] - 9s 7ms/step - loss: 0.2204 - acc: 0.9224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoXf0iM-JMI5"
      },
      "source": [
        "# Character Level CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBGiw_ReuH3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebac3d87-27fa-43f5-fa01-d73cfce474cb"
      },
      "source": [
        "EMBED_DIM_CHAR, VOCAB_SIZE_CHAR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69, 70)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWm7vNXOJLnX"
      },
      "source": [
        "conv_layers_sm = [[256, 7, 3],\n",
        "                  [256, 7, 3],\n",
        "                  [256, 3, -1],\n",
        "                  [256, 3, -1],\n",
        "                  [256, 3, -1],\n",
        "                  [256, 3, 3]]\n",
        "\n",
        "fully_connected_layers_sm = [1024, 1024]\n",
        "\n",
        "num_of_classes = 2\n",
        "dropout_p = 0.5\n",
        "optimizer = 'adam'\n",
        "loss = 'binary_crossentropy'\n",
        "EMBED_DIM_CHAR = len(tokenizer2.word_index)\n",
        "VOCAB_SIZE_CHAR = EMBED_DIM_CHAR + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoV_ErsSJLpn",
        "outputId": "688f1e74-b904-443f-ec3c-79c61cde0939"
      },
      "source": [
        "# Model Construction\n",
        "# Input\n",
        "inputs = Input(shape=(MAX_SEQUENCE_LENGTH_CHAR,), name='input', dtype='int64')  # shape=(?, 1014)\n",
        "# Embedding\n",
        "x = Embedding(VOCAB_SIZE_CHAR,\n",
        "              EMBED_DIM_CHAR,\n",
        "              input_length=MAX_SEQUENCE_LENGTH_CHAR,)(inputs)\n",
        "# Conv\n",
        "for filter_num, filter_size, pooling_size in conv_layers_sm:\n",
        "    x = Conv1D(filter_num, filter_size)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    if pooling_size != -1:\n",
        "        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
        "x = Flatten()(x)  # (None, 8704)\n",
        "# Fully connected layers\n",
        "for dense_size in fully_connected_layers_sm:\n",
        "    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
        "    x = Dropout(dropout_p)(x)\n",
        "# Output Layer\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "# Build model\n",
        "model2 = Model(inputs=inputs, outputs=predictions)\n",
        "model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, binary_crossentropy\n",
        "print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 1014)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 1014, 69)          4830      \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 1008, 256)         123904    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1008, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 336, 256)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 330, 256)          459008    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 330, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 110, 256)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 108, 256)          196864    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 108, 256)          0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 106, 256)          196864    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 106, 256)          0         \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 104, 256)          196864    \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 104, 256)          0         \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 102, 256)          196864    \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 102, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 34, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 8704)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              8913920   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,339,743\n",
            "Trainable params: 11,339,743\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcCfnX9QJLr7",
        "outputId": "3563fc34-3a98-4853-d210-a66c4b3bcbef"
      },
      "source": [
        "tensorboard2 = TensorBoard(log_dir='drive/MyDrive/NNDL_Project/Dataset2/CNN_Char_sm_logs/', histogram_freq=0, write_graph=True)\n",
        "early_stopping2 = EarlyStopping(monitor='val_loss', patience = 2, mode = 'min')\n",
        "cp2 = ModelCheckpoint('drive/MyDrive/NNDL_Project/Dataset2/CNN_Char_sm_bestModel.h5', monitor='val_acc', save_best_only=True, mode='max')\n",
        "\n",
        "print('using early stopping strategy')\n",
        "history2 = model2.fit(x_train_pad2, train_classes, validation_data=(x_val_pad2, val_classes), epochs=15, batch_size=128, callbacks = [early_stopping2, cp2, tensorboard2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using early stopping strategy\n",
            "Epoch 1/15\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4364 - accuracy: 0.7772WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "821/821 [==============================] - 207s 214ms/step - loss: 0.4364 - accuracy: 0.7772 - val_loss: 0.2992 - val_accuracy: 0.8668\n",
            "Epoch 2/15\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.8947WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "821/821 [==============================] - 164s 200ms/step - loss: 0.2542 - accuracy: 0.8947 - val_loss: 0.2288 - val_accuracy: 0.9060\n",
            "Epoch 3/15\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9146WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "821/821 [==============================] - 164s 199ms/step - loss: 0.2106 - accuracy: 0.9146 - val_loss: 0.2574 - val_accuracy: 0.8956\n",
            "Epoch 4/15\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.9274WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "821/821 [==============================] - 164s 199ms/step - loss: 0.1812 - accuracy: 0.9274 - val_loss: 0.2336 - val_accuracy: 0.9031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ZiVHKiJLuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c8652a3-326e-4992-f9b4-dfc9f7d91289"
      },
      "source": [
        "results2 = model2.evaluate(x_test_pad2, test_classes, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176/176 [==============================] - 11s 61ms/step - loss: 0.2293 - accuracy: 0.9055\n"
          ]
        }
      ]
    }
  ]
}